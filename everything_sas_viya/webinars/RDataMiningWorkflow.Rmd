---
title: "Data Mining Workflow"
output:
  pdf_document: default
  html_notebook: default
  html_document: default
---

## Set Up the R Notebook for Analysis

```{r setup, results='hide', warning=FALSE}
# Load necessary packages
library('swat')
library('ggplot2')
library('reshape2')
options(cas.print.messages = FALSE)

# Hostname, port, username, password
conn <- CAS(hostname, port, username, password)

# Change the active caslib to public
cas.sessionProp.setSessOpt(conn, caslib = 'public')
```

<br>

## View Data

```{r investigate}
# Create a CAS table for the prepped data set
castbl <- defCasTable(conn, 'hmeq_prepped')

# Print the first few rows
head(castbl)
```

<br>

## Variable Shortcuts
Note: I do not want to hard code any of my variable names.

```{r shortcuts}
# Get variable info and types
colinfo <- head(cas.table.columnInfo(conn, table = 'hmeq_prepped')$ColumnInfo, -1)

# My target variable is the first column
target <- colinfo$Column[1]
vars <- colinfo$Column[-1]
noms <- c(target, subset(colinfo, Type == 'varchar')$Column)

# For models that can inherently handle missing values (ex: Decision Tree)
inputs <- grep('IMP_', vars, value = TRUE, invert = TRUE)
nominals <- grep('IMP_', noms, value = TRUE, invert = TRUE)

# For models that cannot handle missing values (ex: Neural Network)
imp.inputs <- grep('IMP_', vars, value = TRUE)
imp.nominals <- c(target, grep('IMP_', noms, value = TRUE))
```

<br>

# Model Building

## Decision Tree

```{r decision tree, warning=FALSE}
# Load the decsion tree actionset
loadActionSet(conn, 'decisionTree')

# Train the decision tree model
cas.decisionTree.dtreeTrain(conn,
    table    = list(name = 'hmeq_prepped', where = '_PartInd_ = 0'),
    target   = target, 
    inputs   = inputs, 
    nominals = nominals,
    varImp   = TRUE,
    casOut   = list(name = 'dt_model', replace = TRUE)
)
```

<br>

## Random Forest

```{r random forest, warning=F}
# Train the random forest model
cas.decisionTree.forestTrain(conn,
    table    = list(name = 'hmeq_prepped', where = '_PartInd_ = 0'),
    target   = target, 
    inputs   = inputs, 
    nominals = nominals,
    casOut   = list(name = 'rf_model', replace = TRUE)
)
```

<br>

## Gradient Boosting

```{r gradient boosting, warning=F}
# Train the gradient boosting model
cas.decisionTree.gbtreeTrain(conn,
    table    = list(name = 'hmeq_prepped', where = '_PartInd_ = 0'),
    target   = target, 
    inputs   = inputs, 
    nominals = nominals,
    casOut   = list(name = 'gbt_model', replace = TRUE)
)
```

<br>

## Neural Network

```{r neural network, warning=F}
# Load the neuralNet actionset
loadActionSet(conn, 'neuralNet')

# Build a neural network model
cas.neuralNet.annTrain(conn,
    table    = list(name = 'hmeq_prepped', where = '_PartInd_ = 0'),
    target   = target, 
    inputs   = imp.inputs, 
    nominals = imp.nominals,
    casOut   = list(name = 'nn_model', replace = TRUE)
)
```

<br>

## Score the Models

```{r score, results='hide', warning = F}
# Score the models
models <- c('dt','rf','gbt','nn')
scores <- c(cas.decisionTree.dtreeScore, cas.decisionTree.forestScore, 
            cas.decisionTree.gbtreeScore, cas.neuralNet.annScore)
names(scores) <- models

# Function to help automate prediction process on new data
score.params <- function(model){return(list(
    object       = defCasTable(conn, 'hmeq_prepped'),
    modelTable   = list(name = paste0(model, '_model')),
    copyVars     = list(target, '_PartInd_'),
    assessonerow = TRUE,
    casOut       = list(name = paste0(model, '_scored'), replace = T)
))}
lapply(models, function(x) {do.call(scores[[x]], score.params(x))})

```

<br>

## Compare Confusion Matrix

```{r confusion matrix, warning=FALSE}
# Load the percentile actionset for scoring
loadActionSet(conn, 'percentile')

# Useful function for model assessment
assess.model <- function(model){
    cas.percentile.assess(conn,
        table    = list(name = paste0(model,'_scored'), 
                        where = '_PartInd_ = 1'),
        inputs   = paste0('_', model, '_P_           1'),
        response = target,
        event    = '1')
}

model.names <- c('Decision Tree', 'Random Forest', 
                 'Gradient Boosting', 'Neural Network')
roc.df <- data.frame()
for (i in 1:length(models)){
    tmp <- (assess.model(models[i]))$ROCInfo
    tmp$Model <- model.names[i] 
    roc.df <- rbind(roc.df, tmp)
}

# Manipulate the dataframe
compare <- subset(roc.df, CutOff == 0.5)
rownames(compare) <- NULL
compare[,c('Model','TP','FP','FN','TN')]
```

<br>

## Compare Misclassification

```{r missclassification}
# Build a dataframe to compare the misclassification rates
compare$Misclassification <- 1 - compare$ACC
miss <- compare[order(compare$Misclassification), c('Model','Misclassification')]
rownames(miss) <- NULL
miss
```

<br>

## Compare ROC Curve

```{r ROC}
# Add a new column to be used as the ROC curve label
roc.df$Models <- paste(roc.df$Model, round(roc.df$C, 3), sep = ' - ')

# Create the ROC curve
ggplot(data = roc.df[c('FPR', 'Sensitivity', 'Models')], 
       aes(x = as.numeric(FPR), y = as.numeric(Sensitivity), colour = Models)) + 
       geom_line() +
       labs(x = 'False Positive Rate', y = 'True Positive Rate')
```

<br>

## Compare XGBoost Model

```{r xgboost train}
library('xgboost')
suppressPackageStartupMessages(library('caret'))

# Bring data to R client
df <- to.casDataFrame(castbl, obs = nrow(castbl))
df <- df[,c(target, inputs, '_PartInd_')]

# Create dummy variables through one-hot encoding
df.dum <- df[,nominals[-1]]
dummies <- dummyVars('~ .', data = df.dum)
df.ohe <- as.data.frame(predict(dummies, newdata = df))
df.all.combined <- cbind(df[,-c(which(colnames(df) %in% nominals[-1]))], df.ohe)

# Split into training and validation
train <- df.all.combined[df.all.combined['_PartInd_'] == 0,]
valid <- df.all.combined[df.all.combined['_PartInd_'] == 1,]
    
# Train the XGBoost model
bst <- xgboost(
    data = data.matrix(train[,-1]),
    label = data.matrix(train[,1]),
    objective = "binary:logistic",
    nround = 50,
    eta = 0.1,
    subsample = 0.5,
    colsample_bytree = 0.5
)
```


<br>

## Score and Assess XGBoost on Validation Data

```{r xgboost score}
# Create a dataframe with the misclassification rate for XGBoost
pred <- as.numeric(predict(bst, data.matrix(valid[,-1]), missing = 'NAN') > 0.5)
Misclassification <- mean(as.numeric(pred > 0.5) != valid[,1])
xgb <- data.frame(cbind(Model = 'R - XGBoost', Misclassification))
xgb
```

<br>

## Final Assessment with CAS and R Models

```{r assessment}
# Combine the assessments and order by most accurate on validation data
err <- data.frame(rbind(miss, xgb))
err[,-1] <- round(as.numeric(as.character(err[,-1])),7)
err <- err[order(err[,-1]),]
rownames(err) <- NULL
err
```

## Save the CAS Gradient Boosting Model

```{r save model}
# Save the champion model to disk for later use
cas.table.save(conn, table = list(name = 'gbt_model'), name = 'Jesse_SAS_gbt', replace = T)

# Promote the champion model to public memory to share with team
cas.table.promote(conn, name = 'gbt_model', target = 'Jesse_SAS_gbt', targetLib = 'public')

# Save the challenger (XGBoost) model for later use
xgb.save(bst, "Jesse_R_xgb.model")
```

<br>

## End the Session

```{r end session, results='hide'}
# End the session
cas.session.endSession(conn)
```

