{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Sentiment Analysis\n",
    "Businesses and organizations around the world know that the first requirement for success is a happy customer base. For the purpose of identifying customer sentiment, the microblogging service Twitter, with its enormous collection of active users, is a font of knowledge. The most recent release of SAS Viya has added support for Twitter analysis to the DeepLearning action set’s Recurrent Neural Network layer. This recipe shows a pipeline to analyze sentiment in Twitter data using word embeddings and RNNs in SAS. The overall structure of this document and a small amount of the text comes from [1] \n",
    "\n",
    "[1] https://github.com/sassoftware/sas-viya-programming/tree/master/deeplearning/fashion-mnist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import modules and create CAS session\n",
    "- In this code we import the needed modules and cas action sets\n",
    "- We assign values for the cashost, casport, and casauth values \n",
    "- These are then used to establish a CAS session named 's'\n",
    "- We set exception_on_severity to 2 to enable tracebacks for server-side CAS errors\n",
    "- Documentation to [Connect and Start a Session](http://go.documentation.sas.com/?cdcId=pgmsascdc&cdcVersion=9.4_3.3&docsetId=caspg3&docsetTarget=home.htm&locale=en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-09T17:20:02.863983Z",
     "start_time": "2018-08-09T17:20:00.392665Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Added action set 'deeplearn'.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"cas-results-key\"><b>&#167; actionset</b></div>\n",
       "<div class=\"cas-results-body\">\n",
       "<div>deeplearn</div>\n",
       "</div>\n",
       "<div class=\"cas-output-area\"></div>\n",
       "<p class=\"cas-results-performance\"><small><span class=\"cas-elapsed\">elapsed 0.239s</span> &#183; <span class=\"cas-user\">user 0.23s</span> &#183; <span class=\"cas-sys\">sys 0.028s</span> &#183; <span class=\"cas-memory\">mem 2.13MB</span></small></p>"
      ],
      "text/plain": [
       "[actionset]\n",
       "\n",
       " 'deeplearn'\n",
       "\n",
       "+ Elapsed: 0.239s, user: 0.23s, sys: 0.028s, mem: 2.13mb"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import swat\n",
    "from IPython.display import display\n",
    "\n",
    "swat.options.cas.exception_on_severity = 2\n",
    "\n",
    "s = swat.CAS('rdcgrd075.unx.sas.com', 3217,authinfo=r'/u/saleem/.authinfo')\n",
    "\n",
    "s.loadactionset('deeplearn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Glove Embeddings into CAS\n",
    "\n",
    "Semantic word embeddings, the vector encodings of the meaning of words, are the basis of deep learning for text analytics. \n",
    "\n",
    "In this recipe, we use the public domain glove embeddings trained on Twitter available at [2]. We have made changes to the format of the glove embeddings for the purpose of this work.\n",
    "\n",
    "We remove all words with non-ascii characters to make the file more lightweight, as the tweets themelves are ascii.\n",
    "\n",
    "We also remove \", which is a special character in SAS, and change the delimiter to tabs from spaces.\n",
    "\n",
    "<code>cat glove.twitter.100d.txt | grep -v \\\" | grep -Pv  \"[^\\x00-\\x7F]\" > glove.twitter.100d.clean.txt</code>\n",
    "\n",
    "We include the modified glove file as part of this recipe.\n",
    "\n",
    "[2] https://nlp.stanford.edu/projects/glove/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-09T17:20:02.868770Z",
     "start_time": "2018-08-09T17:20:02.865993Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# An example embeddings file;\n",
    "GLOVE_PATH = 'miniglove.tsv'\n",
    "DELIMITER = \"\\t\"\n",
    "dims = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-09T17:20:03.428504Z",
     "start_time": "2018-08-09T17:20:02.870343Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Cloud Analytic Services made the uploaded file available as table GLOVE in caslib CASUSERHDFS(saleem).\n",
      "NOTE: The table GLOVE has been created in caslib CASUSERHDFS(saleem) from binary data uploaded to Cloud Analytic Services.\n"
     ]
    }
   ],
   "source": [
    "glove = s.CASTable('glove', replace=True)\n",
    "\n",
    "glove = s.upload_file(GLOVE_PATH,\n",
    "                      casout=glove,\n",
    "                      importoptions=dict(fileType='csv',\n",
    "                                         delimiter=\"\\t\",\n",
    "                                         varChars=True,\n",
    "                                         getNames=False,\n",
    "                                         vars=[dict(type='varchar')]+[dict(type='double')]*dims))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Twitter data into CAS\n",
    "Direct distribution of Twitter text is a violation of the Twitter terms of service [3]. The appropriate approach is to distribute data in _dehydrated_ form. That is, we may distribute the tweet ids along with our annotations but without the text. Using [4], the user may download the text themselves through the Twitter API using the command below. You can run it right in the browser. The download takes about twelve hours on our machine.\n",
    "\n",
    "[3] https://twitter.com/en/tos\n",
    "\n",
    "[4] https://github.com/aritter/twitter_download\n",
    "\n",
    "[5] https://developer.twitter.com/en/apply/user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-09T17:20:03.465041Z",
     "start_time": "2018-08-09T17:20:03.430390Z"
    }
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/aritter/twitter_download.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The twitter download tool requires an access token. You can get a token by applying for a twitter developer account [5]. Once you have an account, register an app and get your consumer key and your secret key. Once you have these, update twitter_download/download_tweets_api.py and run it. The script will open a web browser for you to log in with your Twitter credentials. It will save a file with your private keys so you only need to do it once. Now you can download the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-09T17:20:03.505760Z",
     "start_time": "2018-08-09T17:20:03.466655Z"
    }
   },
   "outputs": [],
   "source": [
    "!python twitter_download/download_tweets_api.py --dist emoji_sentiment_data_dehydrated.tsv --output emoji_sentiment_data_rehydrated.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we've downloaded the data, we clean it. To create this dataset, we collected 20,000 tweets containing the \":)\" emoticon and 20,000 contanining \":(.\" We labeled these positive and negative respectively. We then removed tweets containing foul language and ended up with roughly 37,500 tweets. This is a noisy way to label the data, and you are likely to get more accurate labels if you sample all tweets and label manually. Nevertheless, it's an excellent method to get a lot of sentiment data quickly with an unrestrictive license. Since we don't want our sentiment analysis tool to simply learn to detect the presence of a smiley face or a frowny face, we scrub the data of these two emoticons. We also normalize whitespace to a single space each and remove all non-ascii characters and quotation marks to avoid confusing the software."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-09T17:20:04.141298Z",
     "start_time": "2018-08-09T17:20:03.507512Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "path = \"emoji_sentiment_data_rehydrated.tsv\"\n",
    "df = pd.read_csv(path,\n",
    "                 delimiter=\"\\t\",\n",
    "                 names=['_Document_', '_Target_', 'slice', 'text'],\n",
    "                 skiprows=1)\n",
    "\n",
    "def clean(tweet):\n",
    "    tweet = re.sub(r'[^\\x00-\\x7F]+', '', tweet)\n",
    "    tweet = re.sub(r\"\\s+\", ' ', tweet)\n",
    "    tweet = re.sub(r\"^[\\\"']\", \"\", tweet)\n",
    "    tweet = tweet.replace(\"\\'\", \"\")\n",
    "    return re.sub(r\"(:\\)+)|:\\(+\", \"\", tweet)\n",
    "\n",
    "\n",
    "df['text'] = df['text'].apply(clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When a user deletes his or her tweet or makes it private it can no longer be downloaded, so one thing that publicly distributed Twitter data does is decay over time. Fortunately, it's simple to get a quick measure of the amount of data that has been lost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-09T17:20:04.165986Z",
     "start_time": "2018-08-09T17:20:04.143286Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.1% of data deleted or made private\n",
      "21014 datapoints in train\n",
      "688 datapoints in dev\n",
      "6376 datapoints in test\n"
     ]
    }
   ],
   "source": [
    "import html\n",
    "\n",
    "count = len(df)\n",
    "lost_count = len(df[df['text'] == \"Not Available\"])\n",
    "print(\"{:.1%} of data deleted or made private\".format(lost_count/count))\n",
    "\n",
    "df = df[df['text'] != \"Not Available\"]\n",
    "for slize in ['train', 'dev', 'test']:\n",
    "    print(\"{} datapoints in {}\".format(len(df[df['slice'] == slize]), slize))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This second preprocessing step is to coerce the format of the data into that used in the GloVe twitter embeddings. Here we use an included python Twitter normalization tool based on the ruby script provided by the GloVe team. [4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-09T17:20:07.349637Z",
     "start_time": "2018-08-09T17:20:04.167589Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Cloud Analytic Services made the uploaded file available as table REVIEWS_TRAIN in caslib CASUSERHDFS(saleem).\n",
      "NOTE: The table REVIEWS_TRAIN has been created in caslib CASUSERHDFS(saleem) from binary data uploaded to Cloud Analytic Services.\n"
     ]
    }
   ],
   "source": [
    "from twitter_glove import normalize\n",
    "\n",
    "\n",
    "def preprocess(tweet):\n",
    "    return normalize(tweet).lower()\n",
    "\n",
    "\n",
    "df['text'] = df['text'].apply(html.unescape)\n",
    "df['text'] = df['text'].apply(preprocess)\n",
    "\n",
    "df = df.drop_duplicates(subset='_Document_')\n",
    "\n",
    "reviews_train = s.CASTable('reviews_train.csv', replace=True)\n",
    "\n",
    "reviews_train = s.upload_frame(df, casout=reviews_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-09T17:20:07.369586Z",
     "start_time": "2018-08-09T17:20:07.351771Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "positive    0.510374\n",
      "negative    0.489626\n",
      "Name: _Target_, dtype: float64\n",
      "\n",
      "dev\n",
      "positive    0.534884\n",
      "negative    0.465116\n",
      "Name: _Target_, dtype: float64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for slize in ['train', 'dev']:\n",
    "    print(slize)\n",
    "    print(df[df['slice'] == slize]['_Target_'].value_counts(True))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize Text\n",
    "This step involves separating the text into tokens, then presenting the result in a table that ApplyWordVector can use.\n",
    "\n",
    "**Term**: The token\n",
    "\n",
    "**Start**: The position of the token in the document. This is used to sort the terms, as ApplyWordVector is designed for use in a parallel environment and cannot rely on inputs coming to it in order.\n",
    "\n",
    "**Document**: The document id. Because the input is given as a single table, this is important to separate one document from another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-09T17:20:13.629179Z",
     "start_time": "2018-08-09T17:20:07.371320Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28078\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table OUT_OFFSET in caslib CASUSERHDFS(saleem).\n",
      "NOTE: The table OUT_OFFSET has been created in caslib CASUSERHDFS(saleem) from binary data uploaded to Cloud Analytic Services.\n"
     ]
    }
   ],
   "source": [
    "print(len(df))\n",
    "\n",
    "df_cleaned = df\n",
    "\n",
    "df_cols = {\n",
    "    \"_Term_\": [],\n",
    "    \"_Start_\": [],\n",
    "    \"_Document_\": []\n",
    "}\n",
    "\n",
    "for i in df_cleaned.index:\n",
    "    term = list(filter(None, df_cleaned['text'].loc[i].split(\" \")))\n",
    "    df_cols[\"_Term_\"].extend(term)\n",
    "    df_cols[\"_Start_\"].extend(range(len(term)))\n",
    "    df_cols[\"_Document_\"].extend([df_cleaned['_Document_'].loc[i]]*len(term))\n",
    "\n",
    "tokenized_df = pd.DataFrame.from_dict(df_cols)[['_Term_',\n",
    "                                                '_Start_',\n",
    "                                                '_Document_']]\n",
    "\n",
    "out_offset = s.CASTable('out_offset', replace=True)\n",
    "\n",
    "out_offset = s.upload_frame(tokenized_df, casout=out_offset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-09T17:20:13.640451Z",
     "start_time": "2018-08-09T17:20:13.631503Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_Term_</th>\n",
       "      <th>_Start_</th>\n",
       "      <th>_Document_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i</td>\n",
       "      <td>0</td>\n",
       "      <td>960953084577972224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>love</td>\n",
       "      <td>1</td>\n",
       "      <td>960953084577972224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;url&gt;</td>\n",
       "      <td>2</td>\n",
       "      <td>960953084577972224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rt</td>\n",
       "      <td>0</td>\n",
       "      <td>945682890389516288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;allcaps&gt;</td>\n",
       "      <td>1</td>\n",
       "      <td>945682890389516288</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      _Term_  _Start_          _Document_\n",
       "0          i        0  960953084577972224\n",
       "1       love        1  960953084577972224\n",
       "2      <url>        2  960953084577972224\n",
       "3         rt        0  945682890389516288\n",
       "4  <allcaps>        1  945682890389516288"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-09T17:20:13.694247Z",
     "start_time": "2018-08-09T17:20:13.642145Z"
    }
   },
   "outputs": [],
   "source": [
    "# vocab = set(tokenized_df['_Term_'].values)\n",
    "# glove = pd.read_csv(GLOVE_PATH,sep=DELIMITER,header=None)\n",
    "# miniglove=glove[glove[0].isin(vocab)]\n",
    "\n",
    "# len(miniglove)/len(glove)\n",
    "# miniglove.to_csv('miniglove.tsv',sep=DELIMITER,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple out of vocabulary test makes for a good sanity check if there are any mismatches in the glove embedding file. It will change due to Twitter decay, but should be roughly between 0.01 and 0.03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-09T17:20:14.016672Z",
     "start_time": "2018-08-09T17:20:13.695918Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01937017733016004"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "vocab = set([item[0] for item in pd.read_csv(\n",
    "    GLOVE_PATH, sep=DELIMITER, header=None, usecols=[0]).values])\n",
    "\n",
    "tokenized_df['_Term_'].apply(lambda word: word not in vocab).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply Word Vector\n",
    "Here we run apply word vector and merge the resulting word sequences with their labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-09T17:20:16.199626Z",
     "start_time": "2018-08-09T17:20:14.018605Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Added action set 'textparse'.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\"><caption>Selected Rows from Table EMBEDDED</caption>\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th title=\"\"></th>\n",
       "      <th title=\"_Document_\">_Document_</th>\n",
       "      <th title=\"_Sequence_length_\">_Sequence_length_</th>\n",
       "      <th title=\"_F_0_0_\">_F_0_0_</th>\n",
       "      <th title=\"_F_0_1_\">_F_0_1_</th>\n",
       "      <th title=\"_F_0_2_\">_F_0_2_</th>\n",
       "      <th title=\"_F_0_3_\">_F_0_3_</th>\n",
       "      <th title=\"_F_0_4_\">_F_0_4_</th>\n",
       "      <th title=\"_F_0_5_\">_F_0_5_</th>\n",
       "      <th title=\"_F_0_6_\">_F_0_6_</th>\n",
       "      <th title=\"_F_0_7_\">_F_0_7_</th>\n",
       "      <th title=\"...\">...</th>\n",
       "      <th title=\"_F_57_90_\">_F_57_90_</th>\n",
       "      <th title=\"_F_57_91_\">_F_57_91_</th>\n",
       "      <th title=\"_F_57_92_\">_F_57_92_</th>\n",
       "      <th title=\"_F_57_93_\">_F_57_93_</th>\n",
       "      <th title=\"_F_57_94_\">_F_57_94_</th>\n",
       "      <th title=\"_F_57_95_\">_F_57_95_</th>\n",
       "      <th title=\"_F_57_96_\">_F_57_96_</th>\n",
       "      <th title=\"_F_57_97_\">_F_57_97_</th>\n",
       "      <th title=\"_F_57_98_\">_F_57_98_</th>\n",
       "      <th title=\"_F_57_99_\">_F_57_99_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.609595e+17</td>\n",
       "      <td>16</td>\n",
       "      <td>0.60470</td>\n",
       "      <td>0.895420</td>\n",
       "      <td>0.27923</td>\n",
       "      <td>0.033489</td>\n",
       "      <td>0.158730</td>\n",
       "      <td>0.185220</td>\n",
       "      <td>0.30722</td>\n",
       "      <td>0.47445</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.610139e+17</td>\n",
       "      <td>18</td>\n",
       "      <td>-0.32202</td>\n",
       "      <td>-0.001638</td>\n",
       "      <td>-0.12868</td>\n",
       "      <td>1.214900</td>\n",
       "      <td>0.253890</td>\n",
       "      <td>0.281980</td>\n",
       "      <td>-0.21904</td>\n",
       "      <td>-0.38038</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9.610606e+17</td>\n",
       "      <td>29</td>\n",
       "      <td>0.60470</td>\n",
       "      <td>0.895420</td>\n",
       "      <td>0.27923</td>\n",
       "      <td>0.033489</td>\n",
       "      <td>0.158730</td>\n",
       "      <td>0.185220</td>\n",
       "      <td>0.30722</td>\n",
       "      <td>0.47445</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9.610957e+17</td>\n",
       "      <td>7</td>\n",
       "      <td>0.63006</td>\n",
       "      <td>0.651770</td>\n",
       "      <td>0.25545</td>\n",
       "      <td>0.018593</td>\n",
       "      <td>0.043094</td>\n",
       "      <td>0.047194</td>\n",
       "      <td>0.23218</td>\n",
       "      <td>0.11613</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9.611704e+17</td>\n",
       "      <td>6</td>\n",
       "      <td>0.63006</td>\n",
       "      <td>0.651770</td>\n",
       "      <td>0.25545</td>\n",
       "      <td>0.018593</td>\n",
       "      <td>0.043094</td>\n",
       "      <td>0.047194</td>\n",
       "      <td>0.23218</td>\n",
       "      <td>0.11613</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 5802 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Selected Rows from Table EMBEDDED\n",
       "\n",
       "     _Document_  _Sequence_length_  _F_0_0_   _F_0_1_  _F_0_2_   _F_0_3_  \\\n",
       "0  9.609595e+17                 16  0.60470  0.895420  0.27923  0.033489   \n",
       "1  9.610139e+17                 18 -0.32202 -0.001638 -0.12868  1.214900   \n",
       "2  9.610606e+17                 29  0.60470  0.895420  0.27923  0.033489   \n",
       "3  9.610957e+17                  7  0.63006  0.651770  0.25545  0.018593   \n",
       "4  9.611704e+17                  6  0.63006  0.651770  0.25545  0.018593   \n",
       "\n",
       "    _F_0_4_   _F_0_5_  _F_0_6_  _F_0_7_    ...      _F_57_90_  _F_57_91_  \\\n",
       "0  0.158730  0.185220  0.30722  0.47445    ...            0.0        0.0   \n",
       "1  0.253890  0.281980 -0.21904 -0.38038    ...            0.0        0.0   \n",
       "2  0.158730  0.185220  0.30722  0.47445    ...            0.0        0.0   \n",
       "3  0.043094  0.047194  0.23218  0.11613    ...            0.0        0.0   \n",
       "4  0.043094  0.047194  0.23218  0.11613    ...            0.0        0.0   \n",
       "\n",
       "   _F_57_92_  _F_57_93_  _F_57_94_  _F_57_95_  _F_57_96_  _F_57_97_  \\\n",
       "0        0.0        0.0        0.0        0.0        0.0        0.0   \n",
       "1        0.0        0.0        0.0        0.0        0.0        0.0   \n",
       "2        0.0        0.0        0.0        0.0        0.0        0.0   \n",
       "3        0.0        0.0        0.0        0.0        0.0        0.0   \n",
       "4        0.0        0.0        0.0        0.0        0.0        0.0   \n",
       "\n",
       "   _F_57_98_  _F_57_99_  \n",
       "0        0.0        0.0  \n",
       "1        0.0        0.0  \n",
       "2        0.0        0.0  \n",
       "3        0.0        0.0  \n",
       "4        0.0        0.0  \n",
       "\n",
       "[5 rows x 5802 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.loadactionset('textparse')\n",
    "\n",
    "embedded = s.CASTable('embedded', replace=True)\n",
    "\n",
    "s.textparse.applyWordVector(\n",
    "    model=glove,\n",
    "    offset=out_offset,\n",
    "    casout=embedded\n",
    ")\n",
    "\n",
    "embedded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_F_n_e_ = the eth feature of the nth token in the sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-09T17:20:16.283597Z",
     "start_time": "2018-08-09T17:20:16.201708Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5800"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_columns = [column for column in embedded.columns if column.startswith('_F')]\n",
    "len(embedding_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step merges the lost target data back onto the newly embedded sentences. The ApplyWordVec action's metadata does not match that of the original cas action, so we start by clearing the column metadata. Then we simply call SWAT's merge action to combine the two tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-09T17:20:20.870871Z",
     "start_time": "2018-08-09T17:20:16.285255Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9450581073760986\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "format_clearer = [dict(name=column, format=\"\") for column in embedded.columns]\n",
    "\n",
    "embedded.table.alterTable(columns=format_clearer)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "embedded_with_additional_data = s.CASTable('embedded_with_additional_data', replace=True)\n",
    "\n",
    "reviews_train.merge(\n",
    "    embedded, \n",
    "    on=\"_Document_\",\n",
    "    casout=embedded_with_additional_data\n",
    ")\n",
    "\n",
    "print(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-09T17:20:20.960246Z",
     "start_time": "2018-08-09T17:20:20.872804Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\"><caption>Selected Rows from Table EMBEDDED_WITH_ADDITIONAL_DATA</caption>\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th title=\"\"></th>\n",
       "      <th title=\"text\">text</th>\n",
       "      <th title=\"_Target_\">_Target_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;user&gt; applied to a job didnt get past the fir...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;user&gt; make sure u eat bb and stay hydrated</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rt &lt;allcaps&gt; &lt;user&gt; : &lt;user&gt; i love her</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rt &lt;allcaps&gt; &lt;user&gt; : he said that he got a co...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rt &lt;allcaps&gt; &lt;user&gt; : you know what i miss ? n...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Selected Rows from Table EMBEDDED_WITH_ADDITIONAL_DATA\n",
       "\n",
       "                                                text  _Target_\n",
       "0  <user> applied to a job didnt get past the fir...  negative\n",
       "1        <user> make sure u eat bb and stay hydrated  negative\n",
       "2            rt <allcaps> <user> : <user> i love her  negative\n",
       "3  rt <allcaps> <user> : he said that he got a co...  negative\n",
       "4  rt <allcaps> <user> : you know what i miss ? n...  negative"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_with_additional_data[['text', '_Target_']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model: Bidirectional Recurrent Neural Networks with Gated Units\n",
    "To understand a recurrent neural network, imagine a single fully connected neural network that is applied to each step of a sequence. The difference between this and a recurrent neural network is that a hidden layer at time t is given as input at time t+1\n",
    "![A diagram of a recurrent neural network](rnn.png \"A Recurrent Neural Network\")\n",
    "\n",
    "One challenge of the recurrent neural network in its simplest form is that it tends to forget prior input quickly. There are a couple different approaches to this issue, Long Short Term Memory networks (LSTMs) being the most well known [6]. We use another approach, the Gated Recurrent Unit (GRU) [7]. We use the GRU because it requires fewer parameters than the LSTM, making it conceptually simpler and less computationally expensive to train, and it tends to give comparable results [8]. Each of these approaches uses \"gates\" to explicitly control the rate at which old information is forgotten and new information is incorporated.\n",
    "\n",
    "![A diagram showing the general details of a gated recurrent unit (GRU)](gru.png \"A Gated Recurrent Unit\")\n",
    "\n",
    "When we generate a representation for each word in a sentence to represent words in their context, a recurrent neural network that reads from left to right will only include context from the left of each word. A bidirectional recurrent neural network (BiRNN) resolves this by using two recurrent neural networks, one operating from left to right, the other from right to left. The final output is the concatenation of these two representations.\n",
    "\n",
    "![A diagram showing the general details of a bidirectional recurrent neural network](birnn.png \"A Bidirectional Recurrent Neural Network\")\n",
    "\n",
    "For sentiment analysis, we import the embedded data, then use a variable number of BiRNN layers to generate a contextualized representation of the word sequence, which we then feed into a forward RNN and take the last hidden state as a summary of the sentence. We feed this to a fully connected neural network to get the output.\n",
    "\n",
    "[6] http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "\n",
    "[7] https://towardsdatascience.com/understanding-gru-networks-2ef37df6c9be\n",
    "\n",
    "[8] https://arxiv.org/abs/1412.3555"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-09T17:20:20.968992Z",
     "start_time": "2018-08-09T17:20:20.962049Z"
    }
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "settings = dict(\n",
    "    n=25,\n",
    "    init='msra',\n",
    "    bidirectional_layers=1,\n",
    "    learning_rate=0.0005,\n",
    "    step_size=20,\n",
    "    thread_minibatch_size=1,\n",
    "    max_epochs=40,\n",
    "    fc_dropout=0.0,\n",
    "    output_dropout=0.0,\n",
    "    recurrent_dropout=0.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-09T17:20:21.262595Z",
     "start_time": "2018-08-09T17:20:20.970595Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"cas-results-key\"><b>&#167; OutputCasTables</b></div>\n",
       "<div class=\"cas-results-body\">\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th title=\"\"></th>\n",
       "      <th title=\"CAS Library\">casLib</th>\n",
       "      <th title=\"Name\">Name</th>\n",
       "      <th title=\"Number of Rows\">Rows</th>\n",
       "      <th title=\"Number of Columns\">Columns</th>\n",
       "      <th title=\"Table\">casTable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CASUSERHDFS(saleem)</td>\n",
       "      <td>sentiment</td>\n",
       "      <td>70</td>\n",
       "      <td>5</td>\n",
       "      <td>CASTable('sentiment', caslib='CASUSERHDFS(sale...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "</div>\n",
       "<div class=\"cas-output-area\"></div>\n",
       "<p class=\"cas-results-performance\"><small><span class=\"cas-elapsed\">elapsed 0.0269s</span> &#183; <span class=\"cas-user\">user 0.052s</span> &#183; <span class=\"cas-sys\">sys 0.087s</span> &#183; <span class=\"cas-memory\">mem 22.8MB</span></small></p>"
      ],
      "text/plain": [
       "[OutputCasTables]\n",
       "\n",
       "                 casLib       Name  Rows  Columns  \\\n",
       " 0  CASUSERHDFS(saleem)  sentiment    70        5   \n",
       " \n",
       "                                             casTable  \n",
       " 0  CASTable('sentiment', caslib='CASUSERHDFS(sale...  \n",
       "\n",
       "+ Elapsed: 0.0269s, user: 0.052s, sys: 0.087s, mem: 22.8mb"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment = s.CASTable('sentiment', replace=True)\n",
    "\n",
    "# Generate the model\n",
    "s.buildmodel(model=sentiment, type='RNN')\n",
    "\n",
    "del sentiment.params.replace\n",
    "\n",
    "# Add the input layer\n",
    "s.addlayer(model=sentiment, name='data', layer=dict(type='input'))\n",
    "\n",
    "\n",
    "# Generate some number of bidirectional layers\n",
    "# This loop will generate however many bidirectional layers are specified in settings\n",
    "output = ['data']\n",
    "for i in range(settings['bidirectional_layers']):\n",
    "    forward_birnn = 'birnn{}'.format(i)\n",
    "    backward_birnn = forward_birnn+'r'\n",
    "\n",
    "    s.addlayer(model=sentiment, name=forward_birnn, srclayers=output,\n",
    "               layer=dict(type='recurrent',\n",
    "                          n=settings['n'],\n",
    "                          init=settings['init'],\n",
    "                          rnnType='GRU',\n",
    "                          outputType='samelength',\n",
    "                          dropout=settings['recurrent_dropout'],\n",
    "                          reverse=False))\n",
    "    s.addlayer(model=sentiment, name=backward_birnn, srclayers=output,\n",
    "               layer=dict(type='recurrent',\n",
    "                          n=settings['n'],\n",
    "                          init=settings['init'],\n",
    "                          rnnType='GRU',\n",
    "                          outputType='samelength',\n",
    "                          dropout=settings['recurrent_dropout'],\n",
    "                          reverse=True))\n",
    "    output = [forward_birnn, backward_birnn]\n",
    "\n",
    "# summary layer\n",
    "s.addlayer(model=sentiment, name='frnn1', srclayers=output,\n",
    "           layer=dict(type='recurrent',\n",
    "                      n=settings['n'],\n",
    "                      init=settings['init'],\n",
    "                      rnnType='GRU',\n",
    "                      dropout=settings['recurrent_dropout'],\n",
    "                      outputType='encoding'))\n",
    "\n",
    "# output fully connected layer\n",
    "s.addlayer(model=sentiment,\n",
    "           name='outlayer',\n",
    "           srclayers=['frnn1'],\n",
    "           layer=dict(type='output'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-09T17:21:04.428974Z",
     "start_time": "2018-08-09T17:20:21.264415Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE:  The Synchronous mode is enabled.\n",
      "NOTE:  The total number of parameters is 24652.\n",
      "NOTE:  The approximate memory cost is 19.00 MB.\n",
      "NOTE:  Loading weights cost       0.00 (s).\n",
      "NOTE:  Initializing each layer cost       0.15 (s).\n",
      "NOTE:  The total number of workers is 4.\n",
      "NOTE:  The total number of threads on each worker is 32.\n",
      "NOTE:  The total mini-batch size per thread on each worker is 1.\n",
      "NOTE:  The maximum mini-batch size across all workers for the synchronous mode is 128.\n",
      "NOTE:  Target variable: _Target_\n",
      "NOTE:  Number of levels for the target variable:      2\n",
      "NOTE:  Levels for the target variable:\n",
      "NOTE:  Level      0: negative\n",
      "NOTE:  Level      1: positive\n",
      "NOTE:  Number of input variables:  5800\n",
      "NOTE:  Number of numeric input variables:   5800\n",
      "NOTE:  Epoch           Learning Rate     Loss    Fit Error   Validation Loss   Validation Error    Time (s)\n",
      "NOTE:          0          0.0005       0.5649      0.291          0.5559           0.2791           0.92\n",
      "NOTE:          1          0.0005        0.451     0.2081          0.4893           0.2355           0.92\n",
      "NOTE:          2          0.0005       0.4318     0.1998          0.4509           0.2122           0.95\n",
      "NOTE:          3          0.0005       0.4158     0.1913          0.4587           0.2166           0.93\n",
      "NOTE:          4          0.0005       0.4028     0.1848          0.4694           0.2238           0.93\n",
      "NOTE:          5          0.0005       0.3916      0.181          0.4761           0.2267           0.94\n",
      "NOTE:          6          0.0005       0.3817     0.1754          0.4818           0.2267           0.94\n",
      "NOTE:          7          0.0005       0.3729      0.171          0.4874           0.2282           0.93\n",
      "NOTE:          8          0.0005       0.3648     0.1672          0.4929           0.2253           0.93\n",
      "NOTE:          9          0.0005       0.3571     0.1634          0.4982           0.2137           0.93\n",
      "NOTE:         10          0.0005       0.3496     0.1595          0.5024           0.2108           0.96\n",
      "NOTE:         11          0.0005       0.3424     0.1554          0.5037           0.2093           0.98\n",
      "NOTE:         12          0.0005       0.3353     0.1516          0.5011           0.2122           0.96\n",
      "NOTE:         13          0.0005       0.3282     0.1482          0.4959           0.2078           0.95\n",
      "NOTE:         14          0.0005       0.3212     0.1441          0.4896           0.1962           0.95\n",
      "NOTE:         15          0.0005       0.3143     0.1413          0.4831           0.1933           1.00\n",
      "NOTE:         16          0.0005       0.3075     0.1376          0.4767           0.1919           0.98\n",
      "NOTE:         17          0.0005        0.301     0.1344            0.47            0.189           0.94\n",
      "NOTE:         18          0.0005       0.2946     0.1305          0.4633           0.1846           0.95\n",
      "NOTE:         19          0.0005       0.2882     0.1272          0.4573           0.1773           0.95\n",
      "NOTE:         20          0.0003       0.2799     0.1231          0.3973           0.1642           0.97\n",
      "NOTE:         21          0.0003       0.2741     0.1195          0.3967           0.1613           0.96\n",
      "NOTE:         22          0.0003       0.2703     0.1175          0.3981           0.1599           0.97\n",
      "NOTE:         23          0.0003       0.2669     0.1154          0.3996           0.1584           1.01\n",
      "NOTE:         24          0.0003       0.2635     0.1144          0.4012           0.1599           0.97\n",
      "NOTE:         25          0.0003       0.2601      0.112          0.4029           0.1584           0.99\n",
      "NOTE:         26          0.0003       0.2569     0.1104          0.4047           0.1541           0.97\n",
      "NOTE:         27          0.0003       0.2536     0.1087          0.4066            0.157           0.99\n",
      "NOTE:         28          0.0003       0.2505     0.1068          0.4086            0.157           0.94\n",
      "NOTE:         29          0.0003       0.2474     0.1054          0.4109            0.157           0.96\n",
      "NOTE:         30          0.0003       0.2443     0.1036          0.4134            0.157           1.20\n",
      "NOTE:         31          0.0003       0.2413      0.102          0.4161           0.1555           0.94\n",
      "NOTE:         32          0.0003       0.2384        0.1           0.419            0.157           0.96\n",
      "NOTE:         33          0.0003       0.2354     0.0985          0.4221           0.1584           0.96\n",
      "NOTE:         34          0.0003       0.2325     0.0971          0.4254           0.1613           0.94\n",
      "NOTE:         35          0.0003       0.2296     0.0959          0.4288           0.1642           0.92\n",
      "NOTE:         36          0.0003       0.2268     0.0943          0.4325           0.1628           0.94\n",
      "NOTE:         37          0.0003       0.2239     0.0933          0.4364           0.1613           0.94\n",
      "NOTE:         38          0.0003       0.2211     0.0912          0.4407           0.1642           0.94\n",
      "NOTE:         39          0.0003       0.2183     0.0893          0.4454           0.1642           0.93\n",
      "NOTE:  The optimization reached the maximum number of epochs.\n",
      "NOTE:  The total time is      38.35 (s).\n"
     ]
    }
   ],
   "source": [
    "trained_weights = s.CASTable('trainedWeights', replace=True)\n",
    "best_weights = s.CASTable('bestWeights', replace=True)\n",
    "\n",
    "shuffled_embedded = s.CASTable('shuffled_embedded',replace=True)\n",
    "\n",
    "s.shuffle(embedded_with_additional_data,casout=shuffled_embedded)\n",
    "\n",
    "embedded_with_additional_data = shuffled_embedded\n",
    "\n",
    "r = embedded_with_additional_data.query(\"slice EQ 'train'\").dlTrain(\n",
    "        model=sentiment,\n",
    "        dataspecs=[\n",
    "            dict(type='numericnominal',\n",
    "                 layer='data',\n",
    "                 data=embedding_columns,\n",
    "                 numnomParms=dict(\n",
    "                 tokenSize=dims, length='_sequence_length_')),\n",
    "            dict(type='numericnominal',\n",
    "                 layer='outlayer',\n",
    "                 data='_Target_',\n",
    "                 nominals='_Target_')\n",
    "        ],\n",
    "        validtable=embedded_with_additional_data.query(\"slice EQ 'dev'\"),\n",
    "        modelWeights=trained_weights,\n",
    "        bestWeights=best_weights,\n",
    "        optimizer=dict(\n",
    "            miniBatchSize=settings['thread_minibatch_size'],\n",
    "            maxEpochs=settings['max_epochs'],\n",
    "            loglevel=2,\n",
    "            algorithm=dict(method='adam',\n",
    "                           beta1=0.9,\n",
    "                           beta2=0.999,\n",
    "                           gamma=0.5,\n",
    "                           learningRate=settings['learning_rate'],\n",
    "                           clipGradMax=100,\n",
    "                           clipGradMin=-100,\n",
    "                           stepSize=settings['step_size'],\n",
    "                           lrPolicy='step'),\n",
    "            dropout=settings['output_dropout']),\n",
    "        seed=12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-09T17:21:04.628117Z",
     "start_time": "2018-08-09T17:21:04.431183Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"cas-results-key\"><b>&#167; ScoreInfo</b></div>\n",
       "<div class=\"cas-results-body\">\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th title=\"\"></th>\n",
       "      <th title=\"Descr\">Descr</th>\n",
       "      <th title=\"Value\">Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Number of Observations Read</td>\n",
       "      <td>6376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Number of Observations Used</td>\n",
       "      <td>6376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Misclassification Error (%)</td>\n",
       "      <td>16.34253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Loss Error</td>\n",
       "      <td>0.388574</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "</div>\n",
       "<div class=\"cas-results-key\"><hr/><b>&#167; OutputCasTables</b></div>\n",
       "<div class=\"cas-results-body\">\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th title=\"\"></th>\n",
       "      <th title=\"CAS Library\">casLib</th>\n",
       "      <th title=\"Name\">Name</th>\n",
       "      <th title=\"Number of Rows\">Rows</th>\n",
       "      <th title=\"Number of Columns\">Columns</th>\n",
       "      <th title=\"Table\">casTable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CASUSERHDFS(saleem)</td>\n",
       "      <td>sentiment_scored</td>\n",
       "      <td>6376</td>\n",
       "      <td>7</td>\n",
       "      <td>CASTable('sentiment_scored', caslib='CASUSERHD...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "</div>\n",
       "<div class=\"cas-output-area\"></div>\n",
       "<p class=\"cas-results-performance\"><small><span class=\"cas-elapsed\">elapsed 0.155s</span> &#183; <span class=\"cas-user\">user 2.8s</span> &#183; <span class=\"cas-sys\">sys 0.199s</span> &#183; <span class=\"cas-memory\">mem 398MB</span></small></p>"
      ],
      "text/plain": [
       "[ScoreInfo]\n",
       "\n",
       "                          Descr         Value\n",
       " 0  Number of Observations Read          6376\n",
       " 1  Number of Observations Used          6376\n",
       " 2  Misclassification Error (%)      16.34253\n",
       " 3                   Loss Error      0.388574\n",
       "\n",
       "[OutputCasTables]\n",
       "\n",
       "                 casLib              Name  Rows  Columns  \\\n",
       " 0  CASUSERHDFS(saleem)  sentiment_scored  6376        7   \n",
       " \n",
       "                                             casTable  \n",
       " 0  CASTable('sentiment_scored', caslib='CASUSERHD...  \n",
       "\n",
       "+ Elapsed: 0.155s, user: 2.8s, sys: 0.199s, mem: 398mb"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_scored = s.CASTable('sentiment_scored', replace=True)\n",
    "\n",
    "r = embedded_with_additional_data.query(\"slice EQ 'test'\").dlScore(\n",
    "        modelTable=sentiment,\n",
    "        initWeights=best_weights,\n",
    "        copyVars=['_Target_', 'text'],\n",
    "        casOut=sentiment_scored,\n",
    "        bufferSize=2)\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have scored our data, we can perform an analysis of its errors. We can generate a confusion matrix in CAS using the crosstab function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-09T17:21:04.806601Z",
     "start_time": "2018-08-09T17:21:04.629695Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th title=\"\"></th>\n",
       "      <th title=\"_Target_\">_Target_</th>\n",
       "      <th title=\"negative\">Col1</th>\n",
       "      <th title=\"positive\">Col2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>negative</td>\n",
       "      <td>2566.0</td>\n",
       "      <td>560.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>482.0</td>\n",
       "      <td>2768.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   _Target_    Col1    Col2\n",
       "0  negative  2566.0   560.0\n",
       "1  positive   482.0  2768.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cmr = sentiment_scored.crosstab(row='_Target_', col='_DL_PredName_')\n",
    "cmr.Crosstab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see here that negative tweets were mistaken for positive tweets with roughly the same frequency as positive for negative.\n",
    "To examine in more detail, we can look at the misclassified tweets. Remember that our distant supervision approach is noisy, so the ground label may not always agree with your intuition. Here's a look at a few of the falsely classified negative tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-09T17:21:04.855272Z",
     "start_time": "2018-08-09T17:21:04.808883Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                       <user> playboy\n",
       "1    <user> <user> thats not good . are they all fr...\n",
       "2    rt <allcaps> <user> : <user> aa <elong> ! i mi...\n",
       "3                          <user> alles oke ? <repeat>\n",
       "4    rt <allcaps> <user> : rip <allcaps> john perry...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_scored.query(\"_Target_ EQ 'negative' AND _DL_PredName_ EQ 'positive'\")['text'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some falsely classified positive tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-09T17:21:04.929010Z",
     "start_time": "2018-08-09T17:21:04.857332Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    <user> \" that sugar film \" is another good eye-opener                                                                \n",
       "1    rt <allcaps> <user> : why governance matters<user> <url> <url>                                                       \n",
       "2    sorry lame excuse <url>                                                                                              \n",
       "3    <user> please fix it as soon as possible , regards                                                                   \n",
       "4    <user> <user> only thing thatll get me through ! <repeat> im finishing half <number> going home and starting drinking\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "sentiment_scored.query(\"_Target_ EQ 'positive' AND _DL_PredName_ EQ 'negative'\")['text'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some work that a user of this recipe could do to further improve the results:\n",
    "1. Collect more data - The advantage of the emoji approach is that it's cheap. We used a simple version of it, but to get more tweets you can include those with image emoji smiley faces such as 😁 and other text versions such as :c) and (O8. Or if you have the data for it, you can just collect more with the same simple smiles and frowns. We intentionally limited our dataset in order to make it quick to download.\n",
    "\n",
    "2. We could also look for a cleaner way to collect data, since especially in the falsely classified negative tweets we are seeing some that probably would have received a different sentiment score from a human tagger. Keep in mind, human labeled data is expensive.\n",
    "\n",
    "3. Tweak the hyperparameters, make custom sentiment-aware embeddings, or make changes to how the normalization is done. Keep in mind if you change the normalization you will likely have to generate your own embedding file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, remember that it's good manners to end your session when you are done with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-09T17:21:04.993254Z",
     "start_time": "2018-08-09T17:21:04.931198Z"
    }
   },
   "outputs": [],
   "source": [
    "s.terminate()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
