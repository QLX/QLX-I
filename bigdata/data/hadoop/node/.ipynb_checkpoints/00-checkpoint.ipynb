{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![images](../../../images/logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Table Of Contents](../../../../index.ipynb)\n",
    "## Master Big Data In Little Time:  Hadoop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Introduction](../00.ipynb) | **[Hadoop: Sizing Requirements](00.ipynb)** | [Hadoop for Windows](01w.ipynb) | [Hadoop for MacOS/Linux](01o.ipynb)\n",
    "\n",
    "- - - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install and Configuration of Hadoop 3.3.0\n",
    "## Setting Up A Single Node Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install or Activate the Latest Java Development Kit\n",
    "Must use minimum JRE 1.8 or Java 11.  \n",
    "\n",
    "<b>For Windows</b> -\n",
    "- Install [Chocolatey](https://chocolatey.org/install) Application Package Manager.\n",
    "- Install OpenJDK 8 from [here](https://adoptopenjdk.net/?variant=openjdk8&jvmVariant=hotspot).\n",
    "- Enable the latest installed version of Java in the Java Control Panel In the Java Control Panel, click on the Java tab. \n",
    "- Click View to display the Java Runtime Environment Settings. \n",
    "- Verify that the latest Java Runtime version is enabled by checking the Enabled box. \n",
    "- Click OK to save settings.\n",
    "\n",
    "### Install and Configuration of Hadoop 3.3.0 (Mac OS X & Linux)\n",
    "<b>For Mac OS X and Linux</b> -\n",
    "- Use <b>Brew</b> via OpenJDK or download from [here](https://adoptopenjdk.net/?variant=openjdk8&jvmVariant=hotspot).\n",
    "- Using <b>Brew</b> or <b>Linuxbrew</b> install [jenv](http://www.jenv.be/) which allows you to choose the Java version you want to use while having multiple versions installed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Hadoop Sizing Requirements](https://dzone.com/articles/hadoop-cluster-capacity-planning#:~:text=RAM%20Requirement%20for%20a%20Data%20Node&text=Therefore%2C%20RAM%20required%20will%20be,in%2Dmemory%20processing%20data%20nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NameNode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following factors should be considered while deploying Hardware for NameNode\n",
    "\n",
    "1. The NameNode handles the data storage on the cluster.\n",
    "\n",
    "2. However the NameNode is one of the Master nodes in HDFS, it is more robust to hardware failures. Meanwhile, the configuration should be reliable to make the Master nodes ready all the time and run critical cluster services.\n",
    "\n",
    "3. The NameNode will store only the metadata about the files in the HDFS and those details will be stored /accessed in the Physical Memory(RAM) of the Master Node System.\n",
    "\n",
    "4. Hence an important part of the NameNode deployment is the RAM configuration. This configuration is directly proportional to the volume of files that which is handled by the HDFS as its directly accessing the RAM for retrieving the required details.\n",
    "\n",
    "The governing factors to decide Resources\n",
    "\n",
    "1. YARN\n",
    "(Yet Another Resource Negotiator) is Hadoop’s cluster resource management system which will manage the use of resources across the cluster based on its availability and storage.\n",
    "\n",
    "**How much Memory / CPU / Disk / Network etc. should be there on NameNode?**\n",
    "\n",
    "1. Hardware configuration of nodes varies from cluster to cluster and it totally depends on the usage of the cluster based on volume /Velocity /variety of data.\n",
    "\n",
    "2. In Some Hadoop clusters if the velocity of data growth is high, then more importance is given to the storage capacity.\n",
    "\n",
    "3. If the processing of data based on time to get the better outcome, then more importance is given to the processing power of nodes.\n",
    "\n",
    "4. In Hadoop, we could use “Commodity Computers” as nodes for processing/storing. Here, Commodity Computers or Nodes does not mean that it is cheap or less powerful hardware. Actually, it means in-expensive computer and where we could fulfill the need for specialized hardware based on requirement.\n",
    "\n",
    "5. Regarding the configuration, the amount of RAM/DISK on name node is directly proportional to the size of the cluster or the Volume and processing time of the data.\n",
    "\n",
    "Here is a sample hardware configuration for NameNode in a “Commodity Computers”.\n",
    "\n",
    "Processors: 2 Quad Core CPUs running @ 2 GHz\n",
    "RAM: 128 GB\n",
    "Disk: 6 x 1TB SATA\n",
    "Network: 10 Gigabit Ethernet\n",
    "\n",
    "- - -\n",
    "\n",
    "1) NameNode is one of the master services of HDFS which supervises the data storage operations performed by the DataNodes.\n",
    "\n",
    "The following factors need to be considered while deploying NameNode:\n",
    "\n",
    "Since NameNode only deals with storing the metadata, it is recommended to be deployed in a highly reliable hardware. A high-reliability hardware ensures the metadata is not lost, otherwise the entire HDFS & Mapreduce would not work without the metadata.\n",
    "\n",
    "Namenode neither stores the business data nor processes them, as a result, it can be deployed in a file system with comparatively lesser capacity as well as in a less powerful processor.\n",
    "2) The yarn demon is intelligent enough to allocate resources equally among the entire cluster. Generally the yarn demon allocates the resource on the datanode cluster which is closer to the InputSplit.\n",
    "\n",
    "3) The amount of RAM on namenode is directly proportional to the size of the cluster. Usually 1 GB of RAM on namenode is recommended per 1 million Blocks of data. So if the data on the datanodes are spread across 10 million blocks, then 10 GB of RAM is recommended on a namenode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataNode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Diskspace Requirements**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the above parameters in hand, we can plan for commodity machines required for the cluster. (These might not be exactly what is required, but after installation, we can fine tune the environment by scaling up/down the cluster.) The nodes that will be required depends on data to be stored/analyzed. \n",
    "\n",
    "By default, the Hadoop ecosystem creates three replicas of data. So if we go with a default value of 3, we need storage of 100TB *3=300 TB for storing data of one year. We have a retention policy of two years, therefore, the storage required will be 1 year data* retaention period=300*2=600 TB.\n",
    "\n",
    "Assume 30% of data is in container storage and 70% of data is in a Snappy compressed Parque format. From various studies, we found that Parquet Snappy compresses data to 70-80%.\n",
    "\n",
    "We have taken it 70%. Here is the storage requirement calculation:\n",
    "\n",
    "total storage required for data =total storage* % in container storage + total storage * %in compressed format*expected compression\n",
    "\n",
    "600*.30+600*.70*(1-.70)=180+420*.30=180+420*.30=306 TB.\n",
    "\n",
    "In addition to the data, we need space for processing/computation the data plus for some other tasks. We need to decide how much should go to the extra space. We also assume that on an average day, only 10% of data is being processed and a data process creates three times temporary data. So, we need around 30% of total storage as extra storage.\n",
    "\n",
    "Hence, the total storage required for data and other activities is 306+306*.30=397.8 TB.\n",
    "\n",
    "As for the data node, JBOD is recommended. We need to allocate 20% of data storage to the JBOD file system. Therefore, the data storage requirement will go up by 20%. Now, the final figure we arrive at is 397.8(1+.20)=477.36 ~ 478 TB.\n",
    "\n",
    "Let's say DS=478 TB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Memory Requirements**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4 GB per parameter:\n",
    "\n",
    "*RAM Required=DataNode process memory+DataNode TaskTracker memory+OS memory+CPU's core number *Memory per CPU core*\n",
    "\n",
    "At the starting stage, we have allocated four GB memory for each parameter, which can be scaled up as required. Therefore, RAM required will be RAM=4+4+4+12*4=60 GB RAM for batch data nodes and RAM=4+4+4+16*4=76 GB for in-memory processing data nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Installation for Mac OS X and Linux via Homebrew and Linuxbrew\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install appropriate [Java version for Hadoop](https://cwiki.apache.org/confluence/display/HADOOP/Hadoop+Java+Versions) via Brew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brew cask install adoptopenjdk11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For installation of Apache Hadoop 3.3.0 for Mac OS X and Linux we will be using Apache Hadoop 3.3.0 which you can manually download from [here](http://apache.mirrors.hoobly.com/hadoop/common/hadoop-3.3.0/hadoop-3.3.0.tar.gz) or use the scripting framework of **Brew**."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "$ brew install hadoop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hadoop will be installed in the following directory"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "/usr/local/Cellar/hadoop/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/usr/local/Cellar/yarn/"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "cd /usr/local/Cellar/hadoop\n",
    "mkdir hdfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![caption](files/dirs.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
